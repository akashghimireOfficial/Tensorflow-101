{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last [turtorial](subclassing(LayerVsModuleVsModel).ipynb) we learned the difference betweem how we we can create custom `layer` by creating a class subclassed by \n",
    "tf.keras.layers.`Layer`, tf.``Module``, tf.keras.``Model``. There we learned how custom layer created with ``Layer`` can take advantages of keras API. So, here we will learn about them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Layer class: the combination of state (weights) and some computation\n",
    "One of the central abstractions in Keras is the Layer class. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs\n",
    " (a \"call\", the layer's forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor=tf.random.normal(shape=(1,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights integrated using add_weights method is \n",
      " [<tf.Variable 'linear/Variable:0' shape=(2, 5) dtype=float32, numpy=\n",
      "array([[-0.02546739, -0.01241049, -0.00837238,  0.02654559, -0.04006542],\n",
      "       [-0.06405389,  0.00607278, -0.02031103,  0.00740079, -0.05004757]],\n",
      "      dtype=float32)>, <tf.Variable 'linear/Variable:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>]\n",
      "\n",
      "The weights integrated using add_weights method is \n",
      " [<tf.Variable 'linear/Variable:0' shape=(2, 5) dtype=float32, numpy=\n",
      "array([[-0.02546739, -0.01241049, -0.00837238,  0.02654559, -0.04006542],\n",
      "       [-0.06405389,  0.00607278, -0.02031103,  0.00740079, -0.05004757]],\n",
      "      dtype=float32)>, <tf.Variable 'linear/Variable:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"add_weights()\n",
    "Why do we use add_weights() method? Why not simply use tf.random_normal()?  \n",
    "==> The reason is when we create weights using add_weights method the weights becomes part of the layers. That means we can calculate the gradient descent gradient in the training \n",
    "using tape.gradient(loss,model.trainable_weights).\n",
    "\n",
    "We can also create non trainable weights(not tracked during backpropagation.)\n",
    "\"\"\";\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,out_units):\n",
    "        super(Linear,self).__init__()\n",
    "        self.out_units=out_units\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.out_units),initializer='random_normal',trainable=True)\n",
    "        \"\"\"\n",
    "        Here, the weight variable created using .add_weight() can use all the method that a normal tf.Variable can use. For example, .assign_add() etc. \n",
    "        \"\"\"\n",
    "       \n",
    "        self.b=self.add_weight(shape=(input_shape[0],self.out_units),initializer='zeros',trainable=True) \n",
    "\n",
    "    def call(self,x):\n",
    "        \n",
    "        return tf.matmul(x,self.w)+self.b\n",
    "    \n",
    "\n",
    "model_dense=Linear(5)\n",
    "model_dense(tensor)\n",
    "\n",
    "print(\"The weights integrated using add_weights method is \\n\", model_dense.weights)\n",
    "print(\"\\nThe weights integrated using add_weights method is \\n\", model_dense.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2)\n",
      "<tf.Variable 'computesum_5/Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "## adding non trainable weight\n",
    "\n",
    "\n",
    "class computesum(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(computesum,self).__init__()\n",
    "\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        print(input_shape)\n",
    "        self.total=self.add_weight(shape=input_shape,\n",
    "        initializer='zeros',\n",
    "        trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self,x):\n",
    "        print(self.total.shape)\n",
    "        self.total.assign_add(x)\n",
    "        return self.total\n",
    "        \n",
    "print(computesum()(tf.ones((2,2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers are recursively composable\n",
    "This means we can assign instance of custom created layer(Layer in above) to a new custom layer.  It is a best advise to create those sublayers(either custom layers or existing one) in __init__(), and leave it to first __call__() to trigger the building the wights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 3.1300467e-03 -5.5321674e-03 -1.1341706e-02  1.6549388e-02\n",
      "   -1.2665552e-02 -7.5827362e-03  9.3078651e-03 -2.4541318e-03\n",
      "    8.4767072e-04 -3.1936909e-03]\n",
      "  [-7.5282576e-04 -2.2420951e-03 -7.8037851e-03  1.1433904e-02\n",
      "   -6.2538879e-03 -4.7059073e-03  5.4525337e-03 -3.0799480e-03\n",
      "    1.7661857e-04 -1.3994172e-03]\n",
      "  [-6.8967789e-04 -9.7735575e-04 -3.9852289e-03  5.8439542e-03\n",
      "   -2.9316582e-03 -2.3464679e-03  2.6824849e-03 -1.7213271e-03\n",
      "    4.9050548e-05 -6.2887964e-04]]], shape=(1, 3, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class FFN(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(FFN,self).__init__()\n",
    "\n",
    "        self.layer1=Linear(10) ## Customly created\n",
    "        self.layer2=Linear(20)\n",
    "        self.layer3=tf.keras.layers.Dense(10) ## keras defined layers\n",
    "\n",
    "    \n",
    "    def call(self,x):\n",
    "\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ffn_output=FFN()(tensor)\n",
    "\n",
    "print(ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.losses :  [<tf.Tensor: shape=(), dtype=float32, numpy=0.00024027287>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "add_loss(): This method allows you to add custom losses that are specific to the layer or model you are defining.The added losses can be used for various purposes, such as regularization, \n",
    "auxiliary objectives, or other custom objectives that are not directly related to the main output of the model. add_loss() provides flexibility in adding and managing custom losses within the \n",
    "layer or model. These losses are typically accessed and included during training through the model.losses attribute, which collects all the losses added using add_loss().\n",
    "\n",
    "Then, what are some of the benefits of using add_loss() inside the custom layers: \n",
    "- It makes it easier to manage multiple losses. If you have multiple losses, you can simply add them all to the model using the add_loss() function. This makes it easier to track and manage the different losses.\n",
    "\n",
    "- It allows you to customize the way that losses are weighted. When you add a loss to a model using the add_loss() function, you can specify the weight of the loss. This allows you to customize the way that the loss \n",
    "is weighted during training.\n",
    "\n",
    "- It allows you to add losses that are not supported by the tf.keras.Regularization_loss function. The tf.keras.Regularization_loss function only supports L1 and L2 regularization losses.\n",
    " However, you can use the add_loss() function to add any custom loss to a model.\n",
    "\n",
    " We will see how we can use add_loss() during training in another jupyter notebook files. \n",
    "\"\"\";\n",
    "class custom_dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,out_units,alpha=0.01):\n",
    "        super(custom_dense,self).__init__()\n",
    "        self.out_units=out_units\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.out_units),initializer='random_normal',trainable=True)\n",
    "       =self.add_weight(shape=(input_shape[0],self.out_units),initializer='zeros',trainable=True) \n",
    "\n",
    "    def call(self,x):\n",
    "         \n",
    "        regularization_loss = tf.reduce_sum(tf.square(self.w)) * self.alpha\n",
    "        self.add_loss(regularization_loss)\n",
    "        return tf.matmul(x,self.w)+self.b\n",
    "    \n",
    "\n",
    "model_dense=custom_dense(5)\n",
    "model_dense(tensor)\n",
    "\n",
    "print('model.losses : ', model_dense.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 5), dtype=float32, numpy=\n",
       "array([[[-0.0090224 , -0.00258729, -0.01169324,  0.01552782,\n",
       "         -0.00435234],\n",
       "        [ 0.04467518,  0.05567386, -0.1724642 ,  0.05668667,\n",
       "         -0.08487829],\n",
       "        [ 0.01611871, -0.00021536,  0.04688975, -0.0428163 ,\n",
       "          0.01978744]]], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "get_config() : The get_config() method in a custom tf.keras.layers.Layer subclass is used to return a dictionary that represents the configuration of the layer. \n",
    "This configuration dictionary can be used to save and restore the layer, or to create a new layer with the same configuration.\n",
    "\n",
    "The get_config() method should return a dictionary that contains the following keys:\n",
    "name: The name of the layer.\n",
    "**kwargs: Any keyword arguments that were passed to the layer constructor\n",
    "\n",
    "\n",
    "Why get_config() method is useful? \n",
    "\n",
    "- Saving and restoring layers: The get_config() method can be used to save and restore layers. When a layer is saved, the get_config() method is called to get the configuration of the layer. \n",
    "The configuration is then stored in the Keras model file. When the model is restored, the get_config() method is called to create a new layer with the same configuration.\n",
    "\n",
    "- Creating new layers with the same configuration: The get_config() method can be used to create new layers with the same configuration. For example, you can use the get_config() method to \n",
    "create a new layer that has the same configuration as a layer that you have already trained.\n",
    "\n",
    "- Serializing and deserializing layers: The get_config() method can be used to serialize and deserialize layers. Serialization is the process of converting a layer to a format that can be stored or transmitted. \n",
    "Deserialization is the process of converting a layer from a stored or transmitted format to a layer object.\n",
    "\"\"\";\n",
    "\n",
    "class custom_dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,out_units,alpha=0.01,name='test_model'):\n",
    "        super(custom_dense,self).__init__(name=name)\n",
    "        self.out_units=out_units\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.out_units),initializer='random_normal',trainable=True)\n",
    "        self.b=self.add_weight(shape=(input_shape[0],self.out_units),initializer='zeros',trainable=True) \n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config={\n",
    "            'name': self.name,\n",
    "            'alpha': self.alpha,\n",
    "            'units': self.out_units\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,x):\n",
    "        \n",
    "        regularization_loss = tf.reduce_sum(tf.square(self.w)) * self.alpha\n",
    "        self.add_loss(regularization_loss)\n",
    "        return tf.matmul(x,self.w)+self.b\n",
    "    \n",
    "    \n",
    "model_dense=custom_dense(5)\n",
    "model_dense(tensor)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new_model seqential model with above defined subclass custom layers. And we will know how we can use these configs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.keras.Input(shape=(3,2))\n",
    "x=model_dense(inputs)\n",
    "outputs=tf.keras.layers.Dense(10)(x)\n",
    "model=tf.keras.Model(inputs,outputs,name='okokok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try tomorrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Python Projects\\Tensorflow-101\\tf_keras\\creating_a_subclass(custom)_layer.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mmodel_with_config/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Akash Ghimire\\anaconda3\\envs\\vision\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Akash Ghimire\\anaconda3\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\trackable\\trackable_utils.py:126\u001b[0m, in \u001b[0;36mescape_local_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mescape_local_name\u001b[39m(name):\n\u001b[0;32m    121\u001b[0m   \u001b[39m# We need to support slashes in local names for compatibility, since this\u001b[39;00m\n\u001b[0;32m    122\u001b[0m   \u001b[39m# naming scheme is being patched in to things like Layer.add_variable where\u001b[39;00m\n\u001b[0;32m    123\u001b[0m   \u001b[39m# slashes were previously accepted. We also want to use slashes to indicate\u001b[39;00m\n\u001b[0;32m    124\u001b[0m   \u001b[39m# edges traversed to reach the variable, so we escape forward slashes in\u001b[39;00m\n\u001b[0;32m    125\u001b[0m   \u001b[39m# names.\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m   \u001b[39mreturn\u001b[39;00m (name\u001b[39m.\u001b[39;49mreplace(_ESCAPE_CHAR, _ESCAPE_CHAR \u001b[39m+\u001b[39m _ESCAPE_CHAR)\u001b[39m.\u001b[39mreplace(\n\u001b[0;32m    127\u001b[0m       \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, _ESCAPE_CHAR \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "model.save('model_with_config/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_4 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Python Projects\\Tensorflow-101\\tf_keras\\creating_a_subclass(custom)_layer.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     custom_dense(\u001b[39m100\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Restore the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/Tensorflow-101/tf_keras/creating_a_subclass%28custom%29_layer.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Akash Ghimire\\anaconda3\\envs\\vision\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Akash Ghimire\\anaconda3\\envs\\vision\\lib\\site-packages\\keras\\engine\\training.py:3467\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3456\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   3458\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3459\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[0;32m   3460\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3465\u001b[0m     \u001b[39m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[0;32m   3466\u001b[0m     \u001b[39m# defined.\u001b[39;00m\n\u001b[1;32m-> 3467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3468\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3469\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3470\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3471\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3472\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential_4 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    custom_dense(100, alpha=0.01),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "# Restore the model\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "\n",
    "# Print the configuration of the custom_dense layer\n",
    "print(model.layers[0].get_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
