{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to graphs and tf.function\n",
    "> One of the advantage of tf compared to other deep learning frameworks is because tf is much more flexible when it comes to deployment. \n",
    "<br> </br>\n",
    ">>  So, why are we talking about deployment in this specific turtorial?<br>\n",
    "Because in this turtorial we will be learning tf.Graph which enable tf models to run device without python interpretator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are graphs?\n",
    "In the previous three guides, you ran TensorFlow eagerly. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.\n",
    "\n",
    "While eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. Graph execution means that tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf.Graph or simply a \"graph.\"\n",
    "\n",
    "Graphs are data structures that contain a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations. They are defined in a tf.Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code.\n",
    "\n",
    ">In short, graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices. <br>\n",
    "\n",
    "However, you still want to define your machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and run graph in Tensorflow we use **tf.funtion**,either as a direct call or as *decorator*.\n",
    "> decorator: In python decorator are used to add more meaning to the existing function. It can be thought as passing a certain function as parameter to decorator funtion.\n",
    "<br>\n",
    "decorator_fnc(normal_fnc), here normal fnc are feeded as paramter to decorator fucntion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a Python function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reg_fnc(x,y):\n",
    "    return tf.multiply(x,y)\n",
    "\n",
    "## Creating a funtion using tf.function or creating graphs in simple words\n",
    "graph_fnc=tf.function(reg_fnc)\n",
    "\n",
    "### Creating some tensors \n",
    "\n",
    "x=tf.constant([[1,2,3]])\n",
    "y=tf.constant([[5,6,7]])\n",
    "\n",
    "print(\"Output from regular function : {}\".format(reg_fnc(x,y)))\n",
    "print(\"Output from graph function : {}\".format(graph_fnc(x,y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf.Graphs behind one API (learn more in the Polymorphism section). That is how a Function is able to give you the benefits of graph execution, like speed and deployability (refer to The benefits of graphs above).\n",
    "\n",
    "> tf.function applies to a function and all other functions it calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_function(x, y, b):\n",
    "  x = tf.matmul(x, y)\n",
    "  x = x + b\n",
    "  return x\n",
    "\n",
    "# Use the decorator to make `outer_function` a `Function`.\n",
    "@tf.function\n",
    "def outer_function(x):\n",
    "  y = tf.constant([[2.0], [3.0]])\n",
    "  b = tf.constant(4.0)\n",
    "\n",
    "  return inner_function(x, y, b)\n",
    "\n",
    "# Note that the callable will create a graph that\n",
    "# includes `inner_function` as well as `outer_function`.\n",
    "outer_function(tf.constant([[1.0, 2.0]])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polymorphism: one Function, many graphs\n",
    "> A tf.Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()).\n",
    "Each time you invoke a Function with a set of arguments that can't be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf.Graph specialized to those new arguments. The type specification of a tf.Graph's inputs is known as its input signature or just a signature. For more information regarding when a new tf.Graph is generated and how that can be controlled, go to the Rules of tracing section of the Better performance with tf.function guide.\n",
    "The Function stores the tf.Graph corresponding to that signature in a ConcreteFunction. A ConcreteFunction is a wrapper around a tf.Graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c93af7433719cf61beb232a937287b5f6ac44c5a03632b389ba7312dbdbeed85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
