{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, how are you?'\n",
      "b'Hello'\n"
     ]
    }
   ],
   "source": [
    " ## creatin a simple text tensor\n",
    "\n",
    "## We can do that by using simple tf.Variable \n",
    "\n",
    "str_var=tf.Variable('Hello, how are you?') # We can use sentennce.\n",
    "print(str_var.numpy())\n",
    "\n",
    "\n",
    "str_var=tf.Variable('Hello') # We can use single word or char\n",
    "print(str_var.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We mostly use string Variable in NLP tasks, such as text generation, summarization, classification(sucg as Sentiment Analysis).  Before that we need to learn some preprocessing  for text or string format. \n",
    "\n",
    " - step 1: `Tokenizing`:  Usually, we use string in time-series event. If our input is a char, for example, \"hello\", then each char {h e l l o} are treated as different sequence of event. Or, if our input is simply a sentence then we can treat each words as different tokens. For example if our input is \"Hello world\", then we will have two tokens {\"Hello\", \"world\"}. In NLP we prefer to use `token` word for representing each sequence.\n",
    "\n",
    " - Step 2: `Vectorization or Assigning ID`: Computers only understand number. That means they can't understand \"tokens\" which are either a char or word or even phrases. So, we will represent each of these tokens with some unique ID. This process of assigning ID is knonw as Vectorization.  There are different way to vectorize a token, and they are:\n",
    "    1. Assigning `Int` ID:  In this process of vectorization we assign each token with a \"int\" value.  For example: Orange==> 4, Apple==>5. *Note: This method can't* address the semantic relationship. \n",
    "\n",
    "    2. Assigning `array` or `vector` ID:  In this method rather that assigning a 'int' id to a token we will assign then vectors id. There are various method to do this. One of the simple method is `one_hot_encoding()` the \"INT\" id tokens. However, this method is not good as these vectors are mostly sparse. These methods again may not contain the semantic relation. So, other way of doing it is known as `Embedding.` Unlike `one_hot_encoding` it is not sparse. And each index has some value; you can think of it as output from `softmax`.  For example: Orange==> [0.01,0.002,,0.34, ..........]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us learn how to tokenize string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenizing:  b'Hello'\n"
     ]
    }
   ],
   "source": [
    "tf_chars=tf.Variable('Hello')\n",
    "print('Before tokenizing: ',tf_chars.numpy())\n",
    "\n",
    "## Now in order to tokenize we need to split each character. Each character are then treated as diffetent tokens itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing assigning a unique ID to each token\n",
    "\n",
    "Here, each of our tokens will be `char`. And we will use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we need to tokenize text from each char, for this tf.strings unicode split\n",
    "input_text=tf.Variable('hello, wolrd!')\n",
    "char_token=tf.strings.unicode_split(input_text,input_encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'h' b'e' b'l' b'l' b'o' b',' b' ' b'w' b'o' b'l' b'r' b'd' b'!'], shape=(13,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(char_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'h' b'e' b'l' b'l' b'o' b',' b' ' b'w' b'o' b'l' b'r' b'd' b'!']\n"
     ]
    }
   ],
   "source": [
    "print(char_token.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'a', 'b', 'd', 'e', 'f', 'hello']\n",
      "[0 4 0 0 0 0 0 0 0 0 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "vocab=['a','b','d','e','f', 'hello']\n",
    "\n",
    "\n",
    "id_from_char=tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "\n",
    "\n",
    "## We can also pront how the vocabular if id_from_char look like \n",
    "\n",
    "print(list(id_from_char.get_vocabulary())) ## You will see a certain vocab listed here, '[UNK]' for 0\n",
    "\n",
    "## So this means any unique character which is not in the voab then it will be 0 indexed. \n",
    "\n",
    "ids=id_from_char(char_token)\n",
    "\n",
    "\n",
    "print(ids.numpy()) ## You can observe that if the char is not available in above vocab then it will simply return 0. And if these character are there then \n",
    "#it will return the index of that char in the vocab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ids=id_from_char(['[UNK]'])[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor=tf.SparseTensor(indices=skip_ids,\n",
    "                             values=[-float('inf')]*len(skip_ids),\n",
    "                             dense_shape=[len(id_from_char.get_vocabulary())]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=tf.sparse.to_dense(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-inf   0.   0.   0.   0.   0.   0.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why masking is important? \n",
    "\n",
    "==> it is usually added to predicted logits before going passing through softmax; softmax([-inf)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
       "array([0.        , 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "       0.16666667, 0.16666667], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(mask) ## You can see 0 value in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the tokenization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_from_char(['R','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'Hello Sir', 'damn', 'h']\n",
      "[3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab=['Hello Sir', 'damn','h'] ## Also the vocab can be string or char or even sentence. Example, Hello Sir ==> index will be 0, and 'h' \n",
    "\n",
    "\n",
    "id_from_char=tf.keras.layers.StringLookup(vocabulary=vocab,mask_token=None)\n",
    "\n",
    "## Since the vocab contain both \"word\" and \"char\" lets see how the vocab of our id_from_char looks like this time\n",
    "\n",
    "print(list(id_from_char.get_vocabulary()))\n",
    "\n",
    "ids=id_from_char(char_token)\n",
    "\n",
    "\n",
    "print(ids.numpy()) ## You can observe that if the char is not available in above vocab then it will simply return 0. And if these character are there then \n",
    "#it will return the index of that char in the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=int64, numpy=array([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['Hello Sir', 'damn', 'h']\n",
    "id_from_char = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "char_token = tf.constant([b'h', b'e', b'l', b'l', b'o', b',', b' ', b'w', b'o', b'l', b'r', b'd', b'!'])\n",
    "ids = id_from_char(char_token)\n",
    "print(ids.numpy())  # Should output [4 0 0 0 0 0 0 0 0 0 0 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 0 0 0 0 0 0 0 0 0 0 0 0], shape=(13,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning vector like ID (See Embedding above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(13, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## let us Assign vector like id ; say we want to create vector ID for each tokens;\n",
    "## First method is one-hot-encoding\n",
    "dimension_vec=20\n",
    "one_hot_vec=tf.one_hot(ids,depth=dimension_vec)\n",
    "\n",
    "print(one_hot_vec)\n",
    "\n",
    "## However, this method is not considered a good method  as the data is very sparse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using keras Embedding; the preferred method \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Keras Embedding Layer Parameters\n",
    "\n",
    "The `tf.keras.layers.Embedding` layer is a versatile tool in TensorFlow for converting categorical data, like words or categories, into dense vectors of fixed size. Understanding the parameters of this layer is crucial for effective usage.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "### `input_dim`\n",
    "\n",
    "- **Description:** The size of the vocabulary, representing the number of unique words or categories.\n",
    "- **Type:** Integer\n",
    "- **Example:** `input_dim=10000` (for a vocabulary of 10,000 words); `Also, represents 1000 numbers of neurons input passes`\n",
    "- **Use case:** This parameter sets the dimensionality of the input space and defines how many unique categories the layer should expect.\n",
    "\n",
    "### `output_dim`\n",
    "\n",
    "- **Description:** The dimensionality of the dense embedding vectors.\n",
    "- **Type:** Integer\n",
    "- **Example:** `output_dim=128` (for embedding vectors of size 128)\n",
    "- **Use case:** This parameter determines the size of the dense vector representation for each word or category. It's usually chosen based on hyperparameter tuning.\n",
    "\n",
    "### `input_length` (optional)\n",
    "\n",
    "- **Description:** Length of the input sequences. Required for subsequent layers like `Flatten`.\n",
    "- **Type:** Integer\n",
    "- **Example:** `input_length=20` (for input sequences of length 20)\n",
    "- **Use case:** This parameter is crucial for handling padding and ensuring consistent input sizes if you plan to use layers that require fixed input shapes.\n",
    "\n",
    "### `mask_zero` (optional)\n",
    "\n",
    "- **Description:** If `True`, masks the embedding vectors of padding values (0).\n",
    "- **Type:** Boolean\n",
    "- **Default:** `False`\n",
    "- **Use case:** Useful for sequences with varying lengths, as it prevents padding from affecting the model's learning.\n",
    "\n",
    "### Other Optional Parameters\n",
    "\n",
    "- `embeddings_initializer`: Initializer for embedding weights.\n",
    "- `embeddings_regularizer`: Regularizer for embedding weights.\n",
    "- `embeddings_constraint`: Constraint for embedding weights.\n",
    "- ... (and more)\n",
    "\n",
    "> Mostly, we will only require to parameters : input_dim, output_dim, and mask_zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let us try this code \n",
    "vocab_size=250\n",
    "dimension_vec=20\n",
    "embedding_layers=tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=dimension_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_emb_tokens=embedding_layers(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.0405075  -0.00677788 -0.02825972 -0.03136009  0.00859209 -0.04053806\n",
      "   0.00594764  0.03105075  0.03624289  0.01643424  0.03645463 -0.01212888\n",
      "  -0.03168251 -0.03871802  0.03424617  0.01942654  0.01678016  0.02822009\n",
      "   0.01152705  0.03617643]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]\n",
      " [ 0.04692712  0.02869404 -0.00058623  0.02315534 -0.04674196  0.00691707\n",
      "   0.02365105  0.00329816  0.00645505  0.04939267  0.00649648 -0.01620258\n",
      "  -0.00888444  0.03360401  0.04232829 -0.00540885  0.0287331   0.0251907\n",
      "  -0.0379375   0.01429   ]], shape=(13, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(after_emb_tokens) ## You can observe unlike one_hot it is not sparse\n",
    "# This method can also understand semantics of the whole text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toeknizing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, we have tokenize characters using `unicode_split`, and later we assign id using Stringlookup. But what if we want to tokenize words or subwords? Because in normal scenario, we usually use words to communicate not character. \n",
    "\n",
    "One of the way to do so is use of `TextVectorization()` method . The `TextVectorization` layer in TensorFlow is used for preprocessing and tokenizing text data in natural language processing tasks. It's a versatile tool that can be configured using various parameters and methods to customize the tokenization and vectorization process.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "The `TextVectorization` layer accepts several parameters that control its behavior:\n",
    "\n",
    "1. **max_tokens**: An integer specifying the maximum number of tokens in the vocabulary. Tokens that appear less frequently will be treated as out-of-vocabulary (OOV). If set to `None`, all tokens will be included.\n",
    "\n",
    "2. **standardize**: A string specifying the *text normalization* to be applied to each input text. Options include `\"lower_and_strip_punctuation\"` (default), `\"lowercase\"`, and `None`. (Important step of preprocessing)\n",
    "\n",
    "3. **split**: A string specifying the tokenization method. Options include `\"whitespace\"` (default), `\"characters\"`, and a custom tokenization function.\n",
    "\n",
    "4. **ngrams**: A tuple specifying the n-gram ranges for creating subwords. For example, `ngrams=(2, 3)` would create bi-grams and tri-grams.\n",
    "\n",
    "5. **output_mode**: A string specifying the output format. Options include `\"int\"` (default, outputs integer token IDs), `\"binary\"` (one-hot encoding), `\"count\"` (word frequency), and `\"tf-idf\"`.\n",
    "\n",
    "6. **output_sequence_length**: An integer specifying the length of the output sequences. If set to `None`, sequences will be of variable lengths based on the input text.\n",
    "\n",
    "7. **pad_to_max_tokens**: A boolean indicating whether sequences should be padded to the length specified by `max_tokens`. Only applicable when `output_sequence_length` is set.\n",
    "\n",
    "8. **vocabulary**: A list of strings representing the initial vocabulary. Can be used to provide a predefined vocabulary.\n",
    "\n",
    "9. **name**: A string specifying the name of the layer.\n",
    "\n",
    "## Methods\n",
    "\n",
    "The primary method of the `TextVectorization` layer is its call method:\n",
    "\n",
    "- **`call(inputs)`**: This method tokenizes the input text data and applies the configured transformations. The `inputs` parameter should be a tensor or a list of strings containing the text data to be processed. The method returns tokenized sequences according to the specified output mode and sequence length.\n",
    "\n",
    "- **`adapt(data)`**: Calling adapt() on a TextVectorization layer is an alternative to passing in a precomputed vocabulary on construction via the vocabulary argument. A TextVectorization layer should always be either adapted over a dataset or supplied with a vocabulary. During adapt(), the layer will build a vocabulary of all string tokens seen in the dataset, sorted by occurrence count, with ties broken by sort order of the tokens (high to low). At the end of adapt(), if max_tokens is set, the vocabulary wil be truncated to max_tokens. *This is very similar to passing vocab in `tf.keras.layers.StringLookup`*\n",
    "\n",
    "\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample text data\n",
    "text_data = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another sentence for demonstration.\"\n",
    "]\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "vectorizer = tf.keras.layers.TextVectorization(output_mode=\"int\", output_sequence_length=None)\n",
    "\n",
    "# Adapt the vectorizer to the text data\n",
    "vectorizer.adapt(text_data)\n",
    "\n",
    "# Tokenize and transform the text data\n",
    "tokenized_data = vectorizer(text_data)\n",
    "\n",
    "print(tokenized_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 8 5 2]\n",
      " [7 2 6 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "\n",
    "##unlike unicode_split, textvectorization first create tokens and later create ids as well. \n",
    "text_data = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another sentence demonstration.\"\n",
    "]\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "vectorizer = tf.keras.layers.TextVectorization(output_mode=\"int\", output_sequence_length=None)\n",
    "\n",
    "# Adapt the vectorizer to the text data\n",
    "vectorizer.adapt(text_data)\n",
    "\n",
    "# Tokenize and transform the text data\n",
    "tokenized_data = vectorizer(text_data)\n",
    "\n",
    "print(tokenized_data.numpy()) ## o is used for padding, since the output_seq is not fixxed and avoud creating ragged tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'sentence',\n",
       " 'this',\n",
       " 'is',\n",
       " 'example',\n",
       " 'demonstration',\n",
       " 'another',\n",
       " 'an']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary() ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next jupyternotebook we will Learn more about `TextVectorization` and we will adapt it on `imdb dataset.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
