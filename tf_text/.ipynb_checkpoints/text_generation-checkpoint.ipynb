{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
    "\n",
    "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
    "\n",
    "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Shakespeare dataset\n",
    "\n",
    "Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of characters : 1115394 characters.\n"
     ]
    }
   ],
   "source": [
    "## Reading the Data\n",
    "\n",
    "with open(path_to_file,'rb') as file:\n",
    "    text=file.read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Length of characters : {len(text)} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# printing the first 350 text \n",
    "\n",
    "print(text[:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sorted unique chars : 65\n"
     ]
    }
   ],
   "source": [
    "##  tHE UNIQUE CHAR IN THE FILE\n",
    "vocab=sorted(set(text))\n",
    "print(f'len of sorted unique chars : {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text \n",
    "\n",
    "The `tf.keras,layers.StringLookup` layer can convert each character into a numeric ID. it just needs to be split into token first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted chars:\n",
      " <tf.RaggedTensor [[b'A', b'k', b'a', b's', b'h'],\n",
      " [b'G', b'h', b'i', b'm', b'i', b'r', b'e']]>\n"
     ]
    }
   ],
   "source": [
    "example_text=['Akash','Ghimire']\n",
    "\n",
    "chars=tf.strings.unicode_split(example_text,input_encoding='UTF-8') ## Spliting the text before  \n",
    "\n",
    "print('Splitted chars:\\n',chars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vecotorizing each tokens using tf.keras.StringLookup layer\n",
    "\n",
    "id_from_char=tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[14, 50, 40, 58, 47], [20, 47, 48, 52, 48, 57, 44]]>\n"
     ]
    }
   ],
   "source": [
    "## Now converting the above tokens(splitted chars) to id\n",
    "\n",
    "ids=id_from_char(chars)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOr analysing we need to look the ids back to chars so it is humamn readable form. For this we need use invert=True in `tf.keras.layers.StringLookup`. \n",
    "\n",
    "> Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_from_id=tf.keras.layers.StringLookup(vocabulary=id_from_char.get_vocabulary(),invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_rev=char_from_id(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[True, True, True, True, True],\n",
       " [True, True, True, True, True, True, True]]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars==chars_rev ## Here the reversed character is same as the origibal chars (splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'Akash', b'Ghimire'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The above character are in splitted or tokenized form. So, we can convert them back like regular text. To do this you can use tf.strings.reduce_join\n",
    "tf.strings.reduce_join(chars,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(char_from_id(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prediction task\n",
    "\n",
    "Here, we are trying to predict what will be next character give an input of char or group of chars. The input to the model will be sequqnce of characters, and you train the model to predict the output- the following character each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids=id_from_char(tf.strings.unicode_split(text,input_encoding='UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating tensorflow datasets from these  ids \n",
    "\n",
    "\n",
    "ids_dataset=tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'F'\n",
      "b'i'\n",
      "b'r'\n",
      "b's'\n",
      "b't'\n",
      "b' '\n",
      "b'C'\n",
      "b'i'\n",
      "b't'\n",
      "b'i'\n"
     ]
    }
   ],
   "source": [
    "for id in ids_dataset.take(10):\n",
    "    print(char_from_id(id).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "(100,)\n",
      "b' are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
      "(100,)\n",
      "b\" know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us\"\n",
      "(100,)\n",
      "b\" kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it \"\n",
      "(100,)\n",
      "b'be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor'\n"
     ]
    }
   ],
   "source": [
    "## Sequence lenght or batch_size\n",
    "\n",
    "sequence=100\n",
    "\n",
    "batch_dataset=ids_dataset.batch(batch_size=sequence,drop_remainder=True) ## This is for creating how many chars in a single sequence\n",
    "## We can understand that how many seuqnces as how many tokens or time series event\n",
    "\n",
    "for seq in batch_dataset.take(5):\n",
    "    print(seq.shape)\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating train test split\n",
    "\"\"\"\n",
    "In text generation task, our input will be input_text=text[:-1] and our target will be input_text=text[:-1]. This is because in real task we want to predict next word. \n",
    "So, input will start from first char and the model will predict the second(next) character. Similar for input the last char will be second_last char and prediction model \n",
    "will predict the final char. So, this setup is made to split a text to input_text and output_text.\n",
    "\"\"\";\n",
    "\n",
    "def split_input_tagets(sequence):\n",
    "    input_text=sequence[:-1]\n",
    "    target_text=sequence[1:]\n",
    "    return input_text,target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dataset \n",
    "\n",
    "dataset=batch_dataset.map(split_input_tagets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYo'\n",
      "Target Text: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n"
     ]
    }
   ],
   "source": [
    "for input_text,target_text in dataset.take(1):\n",
    "    print(f'Input Text: {text_from_ids(input_text)}')\n",
    "    print(f'Target Text: {text_from_ids(target_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now preparing dataset for training\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "buffer_size=1000\n",
    "\n",
    "dataset=dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputshape : (64, 99)\n",
      "targetshape : (64, 99)\n"
     ]
    }
   ],
   "source": [
    "for input_text,target_text in dataset.take(1):\n",
    "    print(f'inputshape : {input_text.shape}')\n",
    "    print(f'targetshape : {target_text.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 66\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(id_from_char.get_vocabulary())\n",
    "\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "## \n",
    "embedding_dim=256\n",
    "\n",
    "##\n",
    "rnn_units=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 99)\n",
      "model inputs shape : (64, 99)\n",
      "(64, 99, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        print('model inputs shape :',inputs.shape)\n",
    "      \n",
    "        x = self.embedding(inputs, training=training) ## shape of input : batch_size,seq_len,embedding_dime\n",
    "\n",
    "       \n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        \n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel(vocab_size=vocab_size,\n",
    "                embedding_dim=embedding_dim,\n",
    "                rnn_units=rnn_units)\n",
    "\n",
    "\n",
    "# Test the model (Make sure to replace 'dataset' with your actual dataset)\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'O,,;;h\\n\\nKKFn,,;;k\\nppKi\\n\\npLGd?;Qx\\npp,KK,,d,Ap,Fddd\\n\\nKp,FkEKK,,PevmIRKKKKCM,KK;KKYeOxd,MAVR,e x?Vpaw[UNK]'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let us see what the above prediction looks like \n",
    "\n",
    "text_from_ids(tf.argmax(example_batch_predictions,axis=2)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss  4.189926\n"
     ]
    }
   ],
   "source": [
    "## Seeing the loss on above predictions\n",
    "\n",
    "loss_value=loss(input_example_batch,example_batch_predictions)\n",
    "print('Loss ', loss_value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam',loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting other configurations \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "model inputs shape : (None, 99)\n",
      "model inputs shape : (None, 99)\n",
      " 25/175 [===>..........................] - ETA: 4:17 - loss: 3.9409"
     ]
    }
   ],
   "source": [
    "history=model.fit(dataset,epochs=epochs,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids ## are from stringlookup objects \n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None] # why new slices are added\n",
    "    print('skip ids : ',skip_ids)\n",
    "    sparse_mask = tf.SparseTensor( ## what is this? \n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask) ## Why this?\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "   \n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor() \n",
    "\n",
    "    print('input_ids.shape :',input_ids.shape) ## input_ids is not of shape #batch_size,seq_len,\n",
    "\n",
    "    \n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :] ## why only last toen is used\n",
    "    predicted_logits = predicted_logits/self.temperature ## what is the purpose of temperature? \n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1) ## why random is used instead of argmax? \n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip ids :  tf.Tensor([[0]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, char_from_id, id_from_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]], dtype=int64)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_model.ids_from_chars(['[UNK]'])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape : (1, None)\n",
      "model inputs shape : (1, None)\n",
      "input_ids.shape : (1, None)\n",
      "model inputs shape : (1, None)\n",
      "ROMEO:\n",
      "Talk not him.\n",
      "\n",
      "PETRUCHIO:\n",
      "Why, I, as come on, sir, how have goved home arm it.\n",
      "\n",
      "LUCENTIO:\n",
      "Lucentio, and we deter.\n",
      "\n",
      "SEBASTIAN:\n",
      "How said thou art sleeping winder's cham? a crot-beck\n",
      "Against the pining of ready, yield me argo\n",
      "Let hast and one hurble not being eyess possible.\n",
      "\n",
      "Bowimphy Walkings, what is my find me return,\n",
      "And get on me: thou shadies, there were\n",
      "Signior Brainood to her awake: though deputy,\n",
      "Fall doth him and paultiners! let's home? Who's dost, It prays\n",
      "Vingain your goodsless whose valuea, fair sisterous conceased\n",
      "for himself as orne, under him, to a chrintation\n",
      "that he'er and bite thee. Come, a very present,\n",
      "My shame is not so fair; for she doth love\n",
      "Froth, I'll wap add turn be inneced him as speed as\n",
      "I'll know, some bride to authool, I pray.\n",
      "Mastle your tale godden faith.\n",
      "\n",
      "MIRANDA:\n",
      "said, sir, bitue;\n",
      "I have no hast, being over-beatted in.\n",
      "I now we'll not; see where he is not informed,\n",
      "That shades not need but A sick,\n",
      "She's sense your place, rest buy Direst York indeed\n",
      "Lute \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.998962879180908\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_char = tf.constant(['ROMEO:'])\n",
    "token=tf.strings.unicode_split(next_char,input_encoding='UTF-8')\n",
    "ids=id_from_char(token) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, None])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ids=id_from_char(['UNK'])[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[31, 28, 26, 18, 28, 11]], dtype=int64)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ids=id_from_char(['[UNK]'])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]], dtype=int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(id_from_char.get_vocabulary())])\n",
    "prediction_mask = tf.sparse.to_dense(sparse_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x203c2e16ca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
