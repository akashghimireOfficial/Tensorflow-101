{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, how are you?'\n",
      "b'Hello'\n"
     ]
    }
   ],
   "source": [
    " ## creatin a simple text tensor\n",
    "\n",
    "## We can do that by using simple tf.Variable \n",
    "\n",
    "str_var=tf.Variable('Hello, how are you?') # We can use sentennce.\n",
    "print(str_var.numpy())\n",
    "\n",
    "\n",
    "str_var=tf.Variable('Hello') # We can use single word or char\n",
    "print(str_var.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We mostly use string Variable in NLP tasks, such as text generation, summarization, classification(sucg as Sentiment Analysis).  Before that we need to learn some preprocessing  for text or string format. \n",
    "\n",
    " - step 1: `Tokenizing`:  Usually, we use string in time-series event. If our input is a char, for example, \"hello\", then each char {h e l l o} are treated as different sequence of event. Or, if our input is simply a sentence then we can treat each words as different tokens. For example if our input is \"Hello world\", then we will have two tokens {\"Hello\", \"world\"}. In NLP we prefer to use `token` word for representing each sequence.\n",
    "\n",
    " - Step 2: `Vectorization or Assigning ID`: Computers only understand number. That means they can't understand \"tokens\" which are either a char or word or even phrases. So, we will represent each of these tokens with some unique ID. This process of assigning ID is knonw as Vectorization.  There are different way to vectorize a token, and they are:\n",
    "    1. Assigning `Int` ID:  In this process of vectorization we assign each token with a \"int\" value.  For example: Orange==> 4, Apple==>5. *Note: This method can't* address the semantic relationship. \n",
    "\n",
    "    2. Assigning `array` or `vector` ID:  In this method rather that assigning a 'int' id to a token we will assign then vectors id. There are various method to do this. One of the simple method is `one_hot_encoding()` the \"INT\" id tokens. However, this method is not good as these vectors are mostly sparse. These methods again may not contain the semantic relation. So, other way of doing it is known as `Embedding.` Unlike `one_hot_encoding` it is not sparse. And each index has some value; you can think of it as output from `softmax`.  For example: Orange==> [0.01,0.002,,0.34, ..........]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us learn how to tokenize string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenizing:  b'Hello'\n"
     ]
    }
   ],
   "source": [
    "tf_chars=tf.Variable('Hello')\n",
    "print('Before tokenizing: ',tf_chars.numpy())\n",
    "\n",
    "## Now in order to tokenize we need to split each character. Each character are then treated as diffetent tokens itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
