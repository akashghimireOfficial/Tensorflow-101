{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, how are you?'\n",
      "b'Hello'\n"
     ]
    }
   ],
   "source": [
    " ## creatin a simple text tensor\n",
    "\n",
    "## We can do that by using simple tf.Variable \n",
    "\n",
    "str_var=tf.Variable('Hello, how are you?') # We can use sentennce.\n",
    "print(str_var.numpy())\n",
    "\n",
    "\n",
    "str_var=tf.Variable('Hello') # We can use single word or char\n",
    "print(str_var.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We mostly use string Variable in NLP tasks, such as text generation, summarization, classification(sucg as Sentiment Analysis).  Before that we need to learn some preprocessing  for text or string format. \n",
    "\n",
    " - step 1: `Tokenizing`:  Usually, we use string in time-series event. If our input is a char, for example, \"hello\", then each char {h e l l o} are treated as different sequence of event. Or, if our input is simply a sentence then we can treat each words as different tokens. For example if our input is \"Hello world\", then we will have two tokens {\"Hello\", \"world\"}. In NLP we prefer to use `token` word for representing each sequence.\n",
    "\n",
    " - Step 2: `Vectorization or Assigning ID`: Computers only understand number. That means they can't understand \"tokens\" which are either a char or word or even phrases. So, we will represent each of these tokens with some unique ID. This process of assigning ID is knonw as Vectorization.  There are different way to vectorize a token, and they are:\n",
    "    1. Assigning `Int` ID:  In this process of vectorization we assign each token with a \"int\" value.  For example: Orange==> 4, Apple==>5. *Note: This method can't* address the semantic relationship. \n",
    "\n",
    "    2. Assigning `array` or `vector` ID:  In this method rather that assigning a 'int' id to a token we will assign then vectors id. There are various method to do this. One of the simple method is `one_hot_encoding()` the \"INT\" id tokens. However, this method is not good as these vectors are mostly sparse. These methods again may not contain the semantic relation. So, other way of doing it is known as `Embedding.` Unlike `one_hot_encoding` it is not sparse. And each index has some value; you can think of it as output from `softmax`.  For example: Orange==> [0.01,0.002,,0.34, ..........]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us learn how to tokenize string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenizing:  b'Hello'\n"
     ]
    }
   ],
   "source": [
    "tf_chars=tf.Variable('Hello')\n",
    "print('Before tokenizing: ',tf_chars.numpy())\n",
    "\n",
    "## Now in order to tokenize we need to split each character. Each character are then treated as diffetent tokens itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing assigning a unique ID to each token\n",
    "\n",
    "Here, each of our tokens will be `char`. And we will use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'h' b'e' b'l' b'l' b'o' b',' b' ' b'w' b'o' b'l' b'r' b'd' b'!']\n"
     ]
    }
   ],
   "source": [
    "## First we need to tokenize text from each char, for this tf.strings unicode split\n",
    "input_text=tf.Variable('hello, wolrd!')\n",
    "char_token=tf.strings.unicode_split(input_text,input_encoding='UTF-8')\n",
    "print(char_token.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'a', 'b', 'd', 'e', 'f', 'hello']\n",
      "[0 4 0 0 0 0 0 0 0 0 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "vocab=['a','b','d','e','f', 'hello']\n",
    "\n",
    "\n",
    "id_from_char=tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "\n",
    "\n",
    "## We can also pront how the vocabular if id_from_char look like \n",
    "\n",
    "print(list(id_from_char.get_vocabulary())) ## You will see a certain vocab listed here, '[UNK]' for 0\n",
    "\n",
    "## So this means any unique character which is not in the voab then it will be 0 indexed. \n",
    "\n",
    "ids=id_from_char(char_token)\n",
    "\n",
    "\n",
    "print(ids.numpy()) ## You can observe that if the char is not available in above vocab then it will simply return 0. And if these character are there then \n",
    "#it will return the index of that char in the vocab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'Hello Sir', 'damn', 'h']\n",
      "[3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab=['Hello Sir', 'damn','h'] ## Also the vocab can be string or char or even sentence. Example, Hello Sir ==> index will be 0, and 'h' \n",
    "\n",
    "\n",
    "id_from_char=tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "\n",
    "## Since the vocab contain both \"word\" and \"char\" lets see how the vocab of our id_from_char looks like this time\n",
    "\n",
    "print(list(id_from_char.get_vocabulary()))\n",
    "\n",
    "ids=id_from_char(char_token)\n",
    "\n",
    "\n",
    "print(ids.numpy()) ## You can observe that if the char is not available in above vocab then it will simply return 0. And if these character are there then \n",
    "#it will return the index of that char in the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['Hello Sir', 'damn', 'h']\n",
    "id_from_char = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "char_token = tf.constant([b'h', b'e', b'l', b'l', b'o', b',', b' ', b'w', b'o', b'l', b'r', b'd', b'!'])\n",
    "ids = id_from_char(char_token)\n",
    "print(ids.numpy())  # Should output [4 0 0 0 0 0 0 0 0 0 0 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 0 0 0 0 0 0 0 0 0 0 0 0], shape=(13,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning vector like ID (See Embedding above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
