{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.data.Dataset** API support writing descriptive and effficient input pipelines.**Dataset** usage follows a common pattern:\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformation to preprocess the data.\n",
    "3. iterate over the dataset and process the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 20:01:01.898515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:01.929393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:01.929628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:01.930497: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 20:01:01.931124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:01.931294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:01.931363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:02.372094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:02.372245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:02.372321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 20:01:02.372390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4268 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "### Creating dataset from the list \n",
    "dataset=tf.data.Dataset.from_tensor_slices([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(d) : <tensorflow.python.data.ops.dataset_ops._NumpyIterator object at 0x7f603ea719c0>\n",
      "d.next() : 1\n",
      "d.next() : 4\n"
     ]
    }
   ],
   "source": [
    "# as_numpy_iterator() : Return an iterator which converts all elements of the dataset to numpy.\n",
    "d=dataset.as_numpy_iterator()\n",
    "print('type(d) :', d)\n",
    "\n",
    "## we can use next() to iterate d\n",
    "\n",
    "print('d.next() :',d.next())\n",
    "print('d.next() :',d.next())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator next() value [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "## batch(): combines the consecutive elements as batches\n",
    "dataset=tf.data.Dataset.from_tensor_slices(range(15))\n",
    "dataset=dataset.batch(5) ## does not change the original dataset so, must be reassigned\n",
    "iterator=dataset.as_numpy_iterator()\n",
    "print('iterator next() value', iterator.next())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take(number,name)\n",
    "dataset=tf.data.Dataset.range(100)\n",
    "dataset1=dataset.take(5) ## it will take 5 element of dataset to form dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7]\n",
      "[3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "## cache(): caches the element in this dataset. \n",
    "\n",
    "dataset=tf.data.Dataset.range(5)\n",
    "dataset=dataset.map(lambda x: x+3)\n",
    "dataset=dataset.cache()\n",
    "## When the datasets is read first time; then it will generate by range() and map()\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "## After this the iteration is read from the cache directiory\n",
    "\n",
    "print(list(dataset.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Applying transformation using map\n",
    "\n",
    "dataset=dataset.map(lambda x: x**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Listing the element in tf.data.Dataset\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset consisting features and labels\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=tf.random.normal(shape=(2,5))\n",
    "labels=tf.constant(['A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prefetch()\n",
    ">Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "\n",
    "Note: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices(range(10))\n",
    "dataset=dataset.take(3)\n",
    "dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implying what we learned till now on MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformation function \n",
    "\n",
    "def transform(x,y):\n",
    "    \n",
    "    x=tf.cast(x,dtype=tf.float32)/255.0\n",
    "    \n",
    "    x=tf.reshape(x,(784,))\n",
    "    y=tf.one_hot(y,10)\n",
    "    print(x.shape)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "ds_train=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "\n",
    "ds_train=ds_train.map(transform,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train=ds_train.batch(64)\n",
    "ds_train=ds_train.cache()\n",
    "ds_train=ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c93af7433719cf61beb232a937287b5f6ac44c5a03632b389ba7312dbdbeed85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
