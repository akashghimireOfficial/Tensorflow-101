{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.data.Dataset** API support writing descriptive and effficient input pipelines.**Dataset** usage follows a common pattern:\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformation to preprocess the data.\n",
    "3. iterate over the dataset and process the element."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the popular and useful method od tf.data objects are:\n",
    "- as_numpy_iterator()\n",
    "- cache()\n",
    "- shuffle()\n",
    "- batch()\n",
    "- map()\n",
    "- filter()\n",
    "- prefetch()\n",
    "- zip()\n",
    "- take()\n",
    "- skip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "\n",
    "# Dataset is created. Ds is a iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for element in ds: \n",
    "    print(element.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.as_numpy_iterator() : Returns an iterator which convert all elements of the dataset to numpy.\n",
    "\n",
    "npy_iter=ds.as_numpy_iterator()\n",
    "list(ds.as_numpy_iterator()) ## We can also list all the element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for arr in npy_iter:\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "## We can also list all the element in a list\n",
    "\n",
    "print(list(npy_iter)) ## It returns an empty array because all the element has already been iterated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in unbatched ds:  20\n",
      "Number of samples/batches in batched ds:  4\n",
      "tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)\n",
      "tf.Tensor([ 7  8  9 10 11 12], shape=(6,), dtype=int32)\n",
      "tf.Tensor([13 14 15 16 17 18], shape=(6,), dtype=int32)\n",
      "tf.Tensor([19 20], shape=(2,), dtype=int32)\n",
      "\n",
      "[array([1, 2, 3, 4, 5]), array([ 6,  7,  8,  9, 10]), array([11, 12, 13, 14, 15]), array([16, 17, 18, 19, 20])]\n"
     ]
    }
   ],
   "source": [
    "# .batch() \n",
    "\n",
    "# Using this method, we will iterate batches of element in each  iteration\n",
    "\n",
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "\n",
    "print('Number of samples in unbatched ds: ',len(ds))\n",
    "ds_batch=ds.batch(6) ## Here if the last batch does not have 6 samples, then the batch will be created only with left samples\n",
    "print('Number of samples/batches in batched ds: ',len(ds_batch))\n",
    "\n",
    "for element in ds_batch:\n",
    "    print(element)\n",
    "\n",
    "print()\n",
    "print(list(ds.batch(5).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]\n"
     ]
    }
   ],
   "source": [
    "# .map(); similar to map of python\n",
    "\n",
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "ds=ds.map(lambda x: x**2)\n",
    "\n",
    "print(list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    ".filter()\n",
    "\"\"\";\n",
    "\n",
    "ds=tf.data.Dataset.range(10)\n",
    "ds=ds.filter(lambda x: x>4) ## Unlike the map() the defined function will return the boolean value; ds will only contains value(element) which satisfy or return True\n",
    "\n",
    "for element in ds:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#.cache() \n",
    "# This method will cache the iteration of dataset.  The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.\n",
    "# Subsequent iterations will use the cached data. \n",
    "# This method is specially useful when preprocessing can take a lot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(15, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(21, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "ds=ds.map(lambda x: x+3) # We do mapping or other preprocessing technique before caching\n",
    "ds=ds.cache() # Here when we don't provide the filepath for caching the ds then elements are cached() in the the memory.  However, if the ds is too large we will want to cached it into existing folder\n",
    "\n",
    "\n",
    "for element in ds:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "-  .shuffle(buffer_size, seed=None, reshuffle_each_iteration=None, name=None): This method is used to shuffle the ds. \n",
    "Randomly shuffles the elements of this dataset.\n",
    "\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements.\n",
    " For perfect shuffling, a uffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements \n",
    "in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:=\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(3) \n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 2, 0]\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ".prefetch():  Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. \n",
    "This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "\n",
    "Note: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), \n",
    "while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of element in ds is :  100\n",
      "\n",
      "Creating a new ds by taking first twenty samples from ds : 20\n",
      "\n",
      "len of ds_ok : 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    ".take(): From the total ds how many samples you would like to take\n",
    "\"\"\";\n",
    "\n",
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,101))\n",
    "print('The number of element in ds is : ',len(ds))\n",
    "\n",
    "\n",
    "\n",
    "ds_new=ds.take(20)\n",
    "print()\n",
    "print('Creating a new ds by taking first twenty samples from ds :',len(ds_new))\n",
    "\n",
    "\n",
    "\n",
    "## What if we try to take samples more than what existing samples itselfs contains? \n",
    "ds_ok=ds.take(1000) ## Orginally contains only 100 samples\n",
    "print('\\nlen of ds_ok :',len(ds_ok)) ## Len matches the len of original dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[ 6  7  8  9 10]\n",
      "[11 12 13 14 15]\n"
     ]
    }
   ],
   "source": [
    "#take can also be used after batch()\n",
    "\n",
    "ds=tf.data.Dataset.range(1,21)\n",
    "ds_batch=ds.batch(5) ## Total number of batches will be 4(20/5=4)\n",
    "\n",
    "for batch_element in ds_batch.take(3): ## We will only print till first three batches\n",
    "    print(batch_element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Suppose we will like to skip first few samples and take few samples? How will we do that? we can use ds.skip().take()\n",
    "\"\"\";\n",
    "\n",
    "ds=tf.data.Dataset.range(10)\n",
    "\n",
    "ds_new=ds.skip(5).take(5)\n",
    "\n",
    "\n",
    "for element in ds_new:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c93af7433719cf61beb232a937287b5f6ac44c5a03632b389ba7312dbdbeed85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
