{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.data.Dataset** API support writing descriptive and effficient input pipelines.**Dataset** usage follows a common pattern:\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformation to preprocess the data.\n",
    "3. iterate over the dataset and process the element."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the popular and useful method od tf.data objects are:\n",
    "- as_numpy_iterator()\n",
    "- cache()\n",
    "- shuffle()\n",
    "- batch()\n",
    "- map()\n",
    "- prefetch()\n",
    "- zip()\n",
    "- take()\n",
    "- skip()\n",
    "\n",
    "Specially these methods are used in creating tf.data pipeline during training the model in ordered way:\n",
    "- shuffle()\n",
    "- map()\n",
    "- cache()\n",
    "- batch()\n",
    "- prefetch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "\n",
    "# Dataset is created. Ds is a iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for element in ds: \n",
    "    print(element.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.as_numpy_iterator() : Returns an iterator which convert all elements of the dataset to numpy.\n",
    "\n",
    "npy_iter=ds.as_numpy_iterator()\n",
    "list(ds.as_numpy_iterator()) ## We can also list all the element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for arr in npy_iter:\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "## We can also list all the element in a list\n",
    "\n",
    "print(list(npy_iter)) ## It returns an empty array because all the element has already been iterated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n",
      "tf.Tensor([13 14 15 16], shape=(4,), dtype=int32)\n",
      "tf.Tensor([17 18 19 20], shape=(4,), dtype=int32)\n",
      "\n",
      "[array([1, 2, 3, 4, 5]), array([ 6,  7,  8,  9, 10]), array([11, 12, 13, 14, 15]), array([16, 17, 18, 19, 20])]\n"
     ]
    }
   ],
   "source": [
    "# .batch() \n",
    "\n",
    "# Using this method, we will iterate batches of element in each  iteration\n",
    "\n",
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "\n",
    "ds_batch=ds.batch(4)\n",
    "\n",
    "\n",
    "for element in ds_batch:\n",
    "    print(element)\n",
    "\n",
    "print()\n",
    "print(list(ds.batch(5).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]\n"
     ]
    }
   ],
   "source": [
    "# .map(); similar to map of python\n",
    "\n",
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "ds=ds.map(lambda x: x**2)\n",
    "\n",
    "print(list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#.cache() \n",
    "# This method will cache the iteration of dataset.  The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.\n",
    "# Subsequent iterations will use the cached data. \n",
    "# This method is specially useful when preprocessing can take a lot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(15, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(21, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds=tf.data.Dataset.from_tensor_slices(tf.range(1,21))\n",
    "ds=ds.map(lambda x: x+3) # We do mapping or other preprocessing technique before caching\n",
    "ds=ds.cache() # Here when we don't provide the filepath for caching the ds then elements are cached() in the the memory.  However, if the ds is too large we will want to cached it into existing folder\n",
    "\n",
    "\n",
    "for element in ds:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "-  .shuffle(buffer_size, seed=None, reshuffle_each_iteration=None, name=None): This method is used to shuffle the ds. \n",
    "Randomly shuffles the elements of this dataset.\n",
    "\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements.\n",
    " For perfect shuffling, a uffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements \n",
    "in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:=\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 2, 0]\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    ".prefetch():  Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. \n",
    "This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "\n",
    "Note: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), \n",
    "while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n",
    "\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c93af7433719cf61beb232a937287b5f6ac44c5a03632b389ba7312dbdbeed85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
